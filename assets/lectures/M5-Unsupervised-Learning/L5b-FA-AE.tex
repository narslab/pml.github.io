%\documentclass[smaller,handout, dvipsnames]{beamer}
\def\bmode{0} % Mode 0 for presentation, mode 1 for a handout with notes, mode 2 fo% r handout without notes
\if 0\bmode
\immediate\write18{pdflatex -jobname=\jobname-Presentation\space\jobname}
\documentclass[usenames,dvipsnames,smaller]{beamer}
\else \if 1\bmode
\immediate\write18{pdflatex -jobname=\jobname-Handout-Notes\space\jobname}
\documentclass[usenames,dvipsnames,smaller,handout]{beamer}
\usepackage{handoutWithNotes}
\pgfpagesuselayout{2 on 1 with notes}[letterpaper, landscape, border shrink=4mm]
\else \if 2\bmode
\immediate\write18{pdflatex -jobname=\jobname-Handout\space\jobname}
\documentclass[usenames,dvipsnames,smaller,handout]{beamer}
\fi
\fi
\fi


%\setbeamertemplate{section in head/foot}{}
%\setbeamertemplate{section in head/foot shaded}{\textcolor{white}{\insertsectionhead}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\coursetitle}{CEE 616: Probabilistic Machine Learning}
\newcommand{\longlecturetitle}{M5 Unsupervised Learning:\\ 5B:  Factor Analysis and Autoencoders}
\newcommand{\shortlecturetitle}{5B: Factor Analysis and Autoencoders}
\newcommand{\instructor}{Jimi Oke}
\newcommand{\lecturedate}{Dec 4, 2025}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{course-macros.tex}
%\setbeamercolor{local structure}{fg=white}

\begin{document}
\maketitle
\begin{frame}
  \frametitle{Outline}
  \tableofcontents
\end{frame}


 

\section{Factor analysis}

 
\begin{frame}
  \frametitle{Factor analysis model}
  \pe
    \begin{itemize}
  \item Basic idea: there are latent (hidden) \textbf{common factors} $\bm z$ underlying some multivariate observations $\bm x_{n} \in \mb R^{D}$

  \end{itemize}
  \pe
  Factor analysis (FA) is a latent variable generative model specified as:\pe

  \begin{eqnarray}
    p(\bm z) &=& \mc{N}(\bm z|\bm \mu_{0},\bm\Sigma_{0}) \\\pe
    p(\bm x|\bm z,\bm\th) &=& \mc N(\bm x|\bm W\bm z + \bm\mu, \bm \Psi)
  \end{eqnarray}\pe
  where:
  \begin{itemize}
  \item $\bm z$: latent vector \pe
  \item $\bm W$: factor loading matrix, $D\times L$\pe
  \item $\bm \Psi$: diagonal covariance matrix, $D\times D$\pe
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Induced marginal distribution}
  \pe

  \begin{eqnarray}
    p(\bm x|\bm \th) &=&\pe
                         \int \mc{N}(\bm x|\bm W\bm z  + \bm\mu, \bm\Psi)\mc{N}(\bm z|\bm\mu_{0},\bm\Sigma_{0})d\bm z \\\pe
                     &=& \mc{N}(\bm x|\underbrace{\bm W{\rd\bm\mu_{0}} + \bm \mu}_{\text{mean}} ,
                         \underbrace{\bm\Psi + \bm W{\bl\bm\Sigma_{0}}\bm W\tr}_{\text{variance}})
  \end{eqnarray}

  \pe
  Simplifications:
  \begin{itemize}
  \item $\rd\bm\mu_{0} \to \bm 0$ \pe
  \item $\bl\bm\Sigma_{0} \to \bm I$
  \end{itemize}
  \pe
  The simplified marginal distribution then becomes:\pe
  \begin{equation}
    p(\bm x|\bm\th) = \mc{N}(\bm x|\bm \mu, \bm{WW}\tr+\bm\Psi)
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{Generative model simplified}

  \begin{eqnarray}
    \text{[Prior]}\quad   p(\bm z) &=& \mc{N}(\bm z|\bm 0, \bm I) \\\pe
    \text{[Likelihood]}\quad   p(\bm x|\bm z) \pe &=& \mc{N}(\bm x|\bm {Wz} + \bm\mu, \bm\Psi) \\\pe
    \text{[Evidence/marginal]}\quad   p(\bm x) \pe &=& \mc{N}(\bm x|\bm \mu, \bm{WW}\tr + \bm\Psi)
  \end{eqnarray}
\end{frame}

\begin{frame}
  \frametitle{What FA does}
  \pe

  It approximates the covariance matrix of the visible/observed vector $\bm x$ using a low-rank decomposition:\pe

  \begin{equation}
    \bm C = \text{Cov}[\bm x] = \underbrace{\bm{WW}\tr + \bm\Psi}_{\text{low-rank decomp}}
  \end{equation}
  \pe
  \begin{itemize}
  \item $\bm{WW}\tr$ is $D\times D$ (recall: $\bm W \in \mathbb{R}^{D\times L}$) \pe
  \item $\bm\Psi$ is $D\times D$ (restricted to be diagonal) \pe
  \item For each variable $x_{d}$, the \textbf{marginal variance} (each diagonal term in $\bm C$) is given by: \pe
    \begin{equation}
      \mb{V}[x_{d}] = \sum_{k=1}^{L}\underbrace{w_{dk}^{2}}_{\text{common}} + \underbrace{\psi_{d}}_{\text{unique}}
    \end{equation}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Parameters to be estimated}
  \pe
  The unknown parameters in FA are:
  \pe
  \begin{itemize}
  \item $\bm W$: factor loading matrix \pe
  \item $\bm \Psi$: covariance matrix
  \end{itemize}

  \pe
  These can be estimated via:
  \begin{itemize}
  \item Maximum likelihood estimation (MLE) \pe
  \item Expectation-maximization (EM) algorithm \pe
  \item Bayesian methods (e.g., variational inference, MCMC)
  \end{itemize}
\pe

  Once estimated, the \textbf{posterior} of latent embeddings is given by:\pe
  \begin{equation}
    p(\bm z|\bm x) = \mc{N}(\bm z|\bm W\tr \bm C^{-1}(\bm x - \bm\mu),\bm I - \bm{W}\tr\bm{C}^{-1}\bm{W})
  \end{equation}
  \pe
  \begin{itemize}
  \item Posterior has closed-form solution under Gaussian distribution
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{FA model summary}
  \pe
    \begin{eqnarray}
    \text{[Prior]}\quad   p(\bm z) &=& \mc{N}(\bm z|\bm 0, \bm I) \\\pe
    \text{[Likelihood]}\quad   p(\bm x|\bm z) \pe &=& \mc{N}(\bm x|\bm {Wz} + \bm\mu, \bm\Psi) \\\pe
    \text{[Evidence/marginal]}\quad   p(\bm x) \pe &=& \mc{N}(\bm x|\bm \mu, \bm C) \\\pe
     \text{[Posterior]}\quad p(\bm z|\bm x) &=& \mc{N}(\bm z|\bm W\tr \bm C^{-1}(\bm x - \bm\mu),\bm I - \bm{W}\tr\bm{C}^{-1}\bm{W})  
    \end{eqnarray}
    \pe

    \begin{itemize}
    \item $\bm z$: latent vector, length $L$ (assumed to be zero-mean, unit variance) \pe
    \item $\bm x$: observed vector \pe
    \item $\bm W$: $D\times L$ factor loading matrix\pe
    \item $\bm \Psi$: $D\times D$ diagonal covariance matrix or \textbf{matrix of unique variances} \pe
    \item $\bm C = \bm{WW}\tr + \bm\Psi$
    \end{itemize}
\end{frame}
  
\section{FA Estimation}
\begin{frame}
  \frametitle{Unidentifiability of FA parameters}

  \pe

  The parameters $\bm W$ and $\bm\Psi$ are unidentifiable. \pe This can be addressed via: \pe

  \begin{itemize}
  \item Constraining $\bm W$ to have orthonormal columns [PCA] \pe
  \item Constraining $\bm W$ to be lower triangular \pe
  \item Informative rotation: $\tilde{\bm W} = \bm{WR}$, where $\bm R$ is the rotation matrix \pe
    \begin{itemize}
      \item Commonly used rotations: Varimax, Promax, Oblimin, Geomin, Thurstone, Equamax
    \end{itemize}\pe
  \item Sparsity-promoting priors on $\bm W$ \pe
  \item Non-Gaussian priors for latent factors
  \end{itemize}
\end{frame}


% \begin{frame}
%   \frametitle{Rotation}
%   \pe
%   We rotate factors for identifiability and interpretability of factors:
  
%   \begin{itemize}
%   \item Varimax
%   \item Promax
%   \item Oblimin
%   \item Geomin
%   \item Thurstone
%   \item Equamax
%   \end{itemize}
% \end{frame}

\begin{frame}
  \frametitle{PCA as a special case of FA}\pause

  Principal components analysis (PCA) is a special case of FA with:\pe
  \begin{equation}
    \bm\Psi = \sigma^2 \bm I
  \end{equation}
  \pe
  where $\sigma^2$ is the isotropic noise variance. \pe
  In PCA, the covariance of the observed vector is given by:\pe
  \begin{equation}
    \bm C = \bm{WW}\tr + \sigma^2 \bm I
  \end{equation}  

  

\end{frame}

\section{Autoencoders}
\begin{frame}
  \frametitle{Autoencoders as nonlinear PCA/FA}
  \pe
  In PCA/FA, we learn a \textbf{linear mapping} from a high-dimensional observed space $\bm x \in\mb R^D$ to a low-dimensional latent space $\bm z \in\mb R^L$ and vice-versa.
  \pe

  \begin{itemize}
  \item \textbf{Encoder} $f_e$: mapping from $\bm x\to \bm z$ \pe
  \item \textbf{Decoder} $f_d$: mapping from $\bm z \to \bm x$
  \end{itemize}
  \pe
  In PCA, for example, $f_e$ is given by:\pe
  \begin{equation}
    \bm z = \bm W\tr \bm x \equiv f_e(\bm x)
  \end{equation}
  \pe
  And $f_d$ is given by: \pe
  \begin{equation}
    \hat{\bm x} = \bm W \bm z \equiv f_d(\bm z)
  \end{equation}
  To introduce flexibility, we can specify $f_e$ and $f_e$ are nonlinear/more complex functions. This is best accomplished via neural network, resulting in an \textbf{autoencoder} (AE).
\end{frame}

\begin{frame}
  \frametitle{Reconstruction loss}
  \pe
  The reconstruction function is the approximation of the observation from the decoder: \pe
  \begin{equation}
    \hat{\bm x} \equiv r(\bm x) = f_d(f_e(\bm x))
  \end{equation}
  \pe
  An autoencoder is thus trained to minimize the reconstruction loss \pe
  \begin{equation}
    \mc{L}(\bm\th) = ||r(\bm x) - \bm x||_2^2
  \end{equation}
  or equivalently, the negative log-likelihood:\pe
  \begin{equation}
    \mc{L}(\bm\th) = -\log p(\bm x|r(\bm x))
  \end{equation}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Basic autoencoder (AE) architecture}
  \pe

  Autoencoder with 2 single-layer MLPS: input layer, hidden layer (latent representation) and output layer (reconstruction)

  \bigskip
  
  \newcommand{\xin}[2]{$x_#2$}
  \newcommand{\xout}[2]{$\hat x_#2$}
  \newcommand{\z}[2]{$z_#2$}

  \begin{minipage}{.45\linewidth}
\begin{adjustbox}{height= .6\textheight}\centering
  \begin{neuralnetwork}
    
  \tikzstyle{input neuron}=[neuron, fill=orange!70];
  \tikzstyle{output neuron}=[neuron, fill=blue!60!black, text=white];

  \inputlayer[count=7, bias=false, text=\xin]

  % \hiddenlayer[count=5, bias=false]
  % \linklayers

  \hiddenlayer[count=3, bias=false,text  = \z]
  \linklayers

  % \hiddenlayer[count=5, bias=false, text = \z]
  % \linklayers

  \outputlayer[count=7,  text=\xout]
  \linklayers
\end{neuralnetwork}
\end{adjustbox}
\end{minipage}\pe
\begin{minipage}{.45\linewidth}
  \begin{itemize}
  \item Hidden layer (size $L$) is a low-dimensional \textbf{bottleneck} between input and reconstruction \pe
  \item $L\ll D$: undercomplete representation \pe
  \item $L \gg D$: overcomplete representation (regularize to prevent identity learning)
  \end{itemize}
\end{minipage}
\end{frame}


\section{AE variants}
\begin{frame}
  \frametitle{Denoising autoencoders}\pe
  In denoising autoencoders (DAEs), the input is corrupted ($\tilde{\bm x}$) by:
  \begin{itemize}
  \item Gaussian noise: $p_c(\tilde{\bm x}|\bm x) = \mc N(\tilde{\bm x}|\bm x, \sigma^2\bm I)$ \pe

  \item Bernoulli dropout: randomly setting a proportion of input nodes to zero
    
  \end{itemize}



  
    \begin{center}
      \includegraphics[width=.7\textwidth]{dae}

      {\tiny Schematic of a DAE. \\ Source: \url{https://lilianweng.github.io/posts/2018-08-12-vae/}}
    \end{center}

    
  The model is then trained to
  minimize the loss between the reconstructed input $r(\tilde{\bm x})$ and its uncorrupted version $\bm x$
  
\end{frame}

\begin{frame}
  \frametitle{Uses of DAE}
  \pe
  \begin{itemize}
  \item DAEs are used for denoising images \pe
    
  
    \begin{center}
      \includegraphics[width=.5\textwidth]{dae-recon}

      {\tiny Original, corrupted and reconstructed images from MNIST dataset. \\ Source: \url{http://www.opendeep.org/v0.0.5/docs/tutorial-your-first-model}}
    \end{center}

  \item They can also learn vector fields of input data
  \pe
  \item Popular for their simplicity
  \end{itemize}
\end{frame}



\begin{frame}
  \frametitle{Sparse autoencoder (SAE)}

  \pe
  \textbf{Sparse autoencoder} (SAE): sparsity penalty on latent activations \pe

    \begin{equation}
      \Omega(\bm z) = \la ||\bm z||_1
    \end{equation}
    \begin{itemize}
    \item \href{https://arxiv.org/pdf/1312.5663.pdf}{$k$-Sparse autoencoder}: use only $k$ largest activations in training
    \end{itemize}

   
      \begin{center}
      \includegraphics[width=.5\textwidth]{k-sparse-autoencoder}

      {\tiny Filters of the k -sparse autoencoder for different sparsity levels k, learnt from MNIST with 1000 hidden units. \\ Source: \url{https://arxiv.org/pdf/1312.5663.pdf}}
    \end{center}

     Useful for interpretability
  \end{frame}

\begin{frame}
  \frametitle{Other AEs}
  \pe

  \begin{itemize}
  \item \href{http://www.icml-2011.org/papers/455_icmlpaper.pdf}{\textbf{Contractive autoencoder}} (CAE): regularizes via penalty on reconstruction loss \pe

    \begin{equation}
      \Omega(\bm z,\bm x) = \la \Bigg|\Bigg|\fr{\partial f_e(\bm x)}{\partial \bm x} \Bigg|\Bigg|_F^2
    \end{equation}
    \pe

  \item Variational autoencoder (VAE): probablistic version of AE/generative model
  \end{itemize}

\end{frame}

\section{Outlook}
\begin{frame}
  \frametitle{Reading}

  \begin{itemize}
  \item \textbf{PMLI} 20.3
  \item \textbf{DL} 20
  \end{itemize}
\end{frame}
  
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
