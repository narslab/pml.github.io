{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9d1eeb-6011-42c7-8ab0-092975b3995d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BACKPROPAGATION (Reverse-Mode AD)\n",
      "============================================================\n",
      "Input x shape: torch.Size([3])\n",
      "Output y shape: torch.Size([2])\n",
      "\n",
      "Gradients via backprop:\n",
      "  dy/dx shape: torch.Size([3])\n",
      "  dy/dW1 shape: torch.Size([4, 3])\n",
      "  dy/dW2 shape: torch.Size([2, 4])\n",
      "\n",
      "============================================================\n",
      "JACOBIAN-VECTOR PRODUCTS (Forward-Mode AD)\n",
      "============================================================\n",
      "\n",
      "Tangent vector v_x: tensor([1., 0., 0.])\n",
      "\n",
      "JVP result (directional derivative):\n",
      "  Output shape: torch.Size([2])\n",
      "  JVP shape: torch.Size([2])\n",
      "  JVP value: tensor([ 1.4405, -0.6085])\n",
      "\n",
      "============================================================\n",
      "KEY DIFFERENCES\n",
      "============================================================\n",
      "\n",
      "BACKPROPAGATION (VJP - Vector-Jacobian Product):\n",
      "  • Reverse-mode automatic differentiation\n",
      "  • Efficient when outputs << inputs (typical in neural networks)\n",
      "  • Computes: v^T @ J (gradient vector × Jacobian)\n",
      "  • One backward pass gives gradients w.r.t. ALL parameters\n",
      "  • Usage: model.backward(), loss.backward()\n",
      "  \n",
      "JVP (Jacobian-Vector Product):\n",
      "  • Forward-mode automatic differentiation\n",
      "  • Efficient when inputs << outputs\n",
      "  • Computes: J @ v (Jacobian × tangent vector)\n",
      "  • Computes directional derivatives\n",
      "  • Usage: torch.autograd.functional.jvp()\n",
      "  • Useful for: Hessian computation, per-example gradients\n",
      "\n",
      "============================================================\n",
      "COMPUTATIONAL COST COMPARISON\n",
      "============================================================\n",
      "\n",
      "Scenario: 1000 inputs → 10 outputs\n",
      "\n",
      "Backprop (VJP):\n",
      "  • Gradients w.r.t. all inputs: 1 backward pass\n",
      "  • Cost: O(cost of forward pass)\n",
      "\n",
      "Forward-mode (JVP):\n",
      "  • Gradient w.r.t. 1 input direction: 1 forward pass\n",
      "  • Gradients w.r.t. all inputs: 1000 forward passes\n",
      "  • Cost: O(1000 × cost of forward pass)\n",
      "\n",
      "→ Backprop is ~1000× more efficient for this case!\n",
      "\n",
      "============================================================\n",
      "PRACTICAL EXAMPLE: Per-Example Gradients\n",
      "============================================================\n",
      "\n",
      "Batch size: 3\n",
      "Computing per-example directional derivatives using JVP...\n",
      "  Example 0 JVP: tensor([ 0.5270, -1.7870])\n",
      "  Example 1 JVP: tensor([ 0.5270, -1.7870])\n",
      "  Example 2 JVP: tensor([ 0.5270, -1.7870])\n",
      "\n",
      "✓ Demo complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BACKPROPAGATION (Reverse-Mode AD)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple neural network function\n",
    "def network(x, W1, W2):\n",
    "    \"\"\"Two-layer network: y = W2 @ relu(W1 @ x)\"\"\"\n",
    "    h = F.relu(W1 @ x)\n",
    "    y = W2 @ h\n",
    "    return y\n",
    "\n",
    "# Initialize parameters\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "W1 = torch.randn(4, 3, requires_grad=True)\n",
    "W2 = torch.randn(2, 4, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y = network(x, W1, W2)\n",
    "print(f\"Input x shape: {x.shape}\")\n",
    "print(f\"Output y shape: {y.shape}\")\n",
    "\n",
    "# Backpropagation: compute gradients w.r.t. all inputs\n",
    "# This computes VJPs where v is the gradient from the loss\n",
    "loss = y.sum()  # Simple loss for demonstration\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradients via backprop:\")\n",
    "print(f\"  dy/dx shape: {x.grad.shape}\")\n",
    "print(f\"  dy/dW1 shape: {W1.grad.shape}\")\n",
    "print(f\"  dy/dW2 shape: {W2.grad.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"JACOBIAN-VECTOR PRODUCTS (Forward-Mode AD)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reset gradients\n",
    "x.grad = None\n",
    "W1.grad = None\n",
    "W2.grad = None\n",
    "\n",
    "# Define tangent vectors (directions for directional derivatives)\n",
    "v_x = torch.tensor([1.0, 0.0, 0.0])  # Direction in x space\n",
    "v_W1 = torch.zeros_like(W1)\n",
    "v_W2 = torch.zeros_like(W2)\n",
    "\n",
    "print(f\"\\nTangent vector v_x: {v_x}\")\n",
    "\n",
    "# Forward-mode AD using torch.autograd.functional.jvp\n",
    "from torch.autograd.functional import jvp\n",
    "\n",
    "# JVP computes: J(f) @ v where J is the Jacobian\n",
    "def f(x_input):\n",
    "    return network(x_input, W1, W2)\n",
    "\n",
    "output, jvp_result = jvp(f, (x,), (v_x,))\n",
    "\n",
    "print(f\"\\nJVP result (directional derivative):\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  JVP shape: {jvp_result.shape}\")\n",
    "print(f\"  JVP value: {jvp_result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY DIFFERENCES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "BACKPROPAGATION (VJP - Vector-Jacobian Product):\n",
    "  • Reverse-mode automatic differentiation\n",
    "  • Efficient when outputs << inputs (typical in neural networks)\n",
    "  • Computes: v^T @ J (gradient vector × Jacobian)\n",
    "  • One backward pass gives gradients w.r.t. ALL parameters\n",
    "  • Usage: model.backward(), loss.backward()\n",
    "  \n",
    "JVP (Jacobian-Vector Product):\n",
    "  • Forward-mode automatic differentiation\n",
    "  • Efficient when inputs << outputs\n",
    "  • Computes: J @ v (Jacobian × tangent vector)\n",
    "  • Computes directional derivatives\n",
    "  • Usage: torch.autograd.functional.jvp()\n",
    "  • Useful for: Hessian computation, per-example gradients\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPUTATIONAL COST COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_inputs = 1000\n",
    "n_outputs = 10\n",
    "\n",
    "print(f\"\\nScenario: {n_inputs} inputs → {n_outputs} outputs\")\n",
    "print(f\"\\nBackprop (VJP):\")\n",
    "print(f\"  • Gradients w.r.t. all inputs: 1 backward pass\")\n",
    "print(f\"  • Cost: O(cost of forward pass)\")\n",
    "\n",
    "print(f\"\\nForward-mode (JVP):\")\n",
    "print(f\"  • Gradient w.r.t. 1 input direction: 1 forward pass\")\n",
    "print(f\"  • Gradients w.r.t. all inputs: {n_inputs} forward passes\")\n",
    "print(f\"  • Cost: O({n_inputs} × cost of forward pass)\")\n",
    "\n",
    "print(f\"\\n→ Backprop is ~{n_inputs}× more efficient for this case!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PRACTICAL EXAMPLE: Per-Example Gradients\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Demonstrate a use case where JVP is useful\n",
    "batch_size = 3\n",
    "x_batch = torch.randn(batch_size, 3, requires_grad=True)\n",
    "W1_new = torch.randn(4, 3, requires_grad=True)\n",
    "W2_new = torch.randn(2, 4, requires_grad=True)\n",
    "\n",
    "def batch_network(x_batch):\n",
    "    # Process batch\n",
    "    results = []\n",
    "    for i in range(x_batch.shape[0]):\n",
    "        results.append(network(x_batch[i], W1_new, W2_new))\n",
    "    return torch.stack(results)\n",
    "\n",
    "# Using JVP for directional derivatives per example\n",
    "print(f\"\\nBatch size: {batch_size}\")\n",
    "print(\"Computing per-example directional derivatives using JVP...\")\n",
    "\n",
    "for i in range(batch_size):\n",
    "    v = torch.zeros_like(x_batch)\n",
    "    v[i] = torch.ones(3)  # Direction for example i\n",
    "    \n",
    "    _, jvp_i = jvp(batch_network, (x_batch,), (v,))\n",
    "    print(f\"  Example {i} JVP: {jvp_i[i]}\")\n",
    "\n",
    "print(\"\\n✓ Demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nars)",
   "language": "python",
   "name": "nars"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
