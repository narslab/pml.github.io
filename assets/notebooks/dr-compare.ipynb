{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing PCA, Factor Analysis, and Autoencoders\n",
    "\n",
    "This notebook demonstrates three popular dimensionality reduction techniques:\n",
    "- **PCA (Principal Component Analysis)**: Linear transformation that maximizes variance\n",
    "- **FA (Factor Analysis)**: Identifies latent factors that explain correlations\n",
    "- **AE (Autoencoder)**: Neural network that learns nonlinear representations\n",
    "\n",
    "We'll use the MNIST dataset for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset (using a subset for faster computation)\n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "X, y = mnist.data[:5000], mnist.target[:5000]\n",
    "\n",
    "# Convert to numpy arrays and normalize\n",
    "X = np.array(X, dtype=np.float32) / 255.0\n",
    "y = np.array(y, dtype=int)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f\"Label: {y_train[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample MNIST Digits', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA (Principal Component Analysis)\n",
    "\n",
    "PCA finds orthogonal directions of maximum variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with 2 components for visualization\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], \n",
    "                     c=y_train, cmap='tab10', alpha=0.6, s=10)\n",
    "plt.colorbar(scatter, label='Digit')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('PCA: 2D Projection of MNIST')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction with PCA (using more components for better reconstruction)\n",
    "pca_reconstruct = PCA(n_components=50)\n",
    "X_train_pca_50 = pca_reconstruct.fit_transform(X_train)\n",
    "X_reconstructed_pca = pca_reconstruct.inverse_transform(X_train_pca_50)\n",
    "\n",
    "# Calculate reconstruction error\n",
    "pca_mse = np.mean((X_train - X_reconstructed_pca) ** 2)\n",
    "print(f\"PCA Reconstruction MSE (50 components): {pca_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Factor Analysis\n",
    "\n",
    "FA models the data as a linear combination of latent factors plus noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Factor Analysis with 2 components\n",
    "fa = FactorAnalysis(n_components=n_components, random_state=42)\n",
    "X_train_fa = fa.fit_transform(X_train)\n",
    "X_test_fa = fa.transform(X_test)\n",
    "\n",
    "print(f\"Factor Analysis noise variance (first 10 features): {fa.noise_variance_[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FA embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_train_fa[:, 0], X_train_fa[:, 1], \n",
    "                     c=y_train, cmap='tab10', alpha=0.6, s=10)\n",
    "plt.colorbar(scatter, label='Digit')\n",
    "plt.xlabel('First Factor')\n",
    "plt.ylabel('Second Factor')\n",
    "plt.title('Factor Analysis: 2D Projection of MNIST')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction with Factor Analysis\n",
    "fa_reconstruct = FactorAnalysis(n_components=50, random_state=42)\n",
    "X_train_fa_50 = fa_reconstruct.fit_transform(X_train)\n",
    "X_reconstructed_fa = fa_reconstruct.inverse_transform(X_train_fa_50)\n",
    "\n",
    "# Calculate reconstruction error\n",
    "fa_mse = np.mean((X_train - X_reconstructed_fa) ** 2)\n",
    "print(f\"FA Reconstruction MSE (50 components): {fa_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Autoencoder\n",
    "\n",
    "A neural network that learns a compressed representation through backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=784, encoding_dim=2):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, encoding_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Sigmoid()  # Output in [0, 1] range\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "autoencoder = Autoencoder(input_dim=784, encoding_dim=2).to(device)\n",
    "print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, X_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "\n",
    "print(\"Training Autoencoder...\")\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_x, _ in train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = autoencoder(batch_x)\n",
    "        loss = criterion(outputs, batch_x)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Autoencoder Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the data\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    X_train_ae = autoencoder.encode(X_train_tensor.to(device)).cpu().numpy()\n",
    "    X_test_ae = autoencoder.encode(X_test_tensor.to(device)).cpu().numpy()\n",
    "\n",
    "# Visualize AE embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_train_ae[:, 0], X_train_ae[:, 1], \n",
    "                     c=y_train, cmap='tab10', alpha=0.6, s=10)\n",
    "plt.colorbar(scatter, label='Digit')\n",
    "plt.xlabel('First Encoding Dimension')\n",
    "plt.ylabel('Second Encoding Dimension')\n",
    "plt.title('Autoencoder: 2D Projection of MNIST')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction with Autoencoder\n",
    "with torch.no_grad():\n",
    "    X_reconstructed_ae = autoencoder(X_train_tensor.to(device)).cpu().numpy()\n",
    "\n",
    "# Calculate reconstruction error\n",
    "ae_mse = np.mean((X_train - X_reconstructed_ae) ** 2)\n",
    "print(f\"Autoencoder Reconstruction MSE (2D encoding): {ae_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison of Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions\n",
    "n_samples = 5\n",
    "fig, axes = plt.subplots(4, n_samples, figsize=(15, 10))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Original\n",
    "    axes[0, i].imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel('Original', fontsize=12, rotation=0, ha='right')\n",
    "    \n",
    "    # PCA reconstruction\n",
    "    axes[1, i].imshow(X_reconstructed_pca[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('PCA\\n(50 comp)', fontsize=12, rotation=0, ha='right')\n",
    "    \n",
    "    # FA reconstruction\n",
    "    axes[2, i].imshow(X_reconstructed_fa[i].reshape(28, 28), cmap='gray')\n",
    "    axes[2, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[2, i].set_ylabel('FA\\n(50 comp)', fontsize=12, rotation=0, ha='right')\n",
    "    \n",
    "    # AE reconstruction\n",
    "    axes[3, i].imshow(X_reconstructed_ae[i].reshape(28, 28), cmap='gray')\n",
    "    axes[3, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[3, i].set_ylabel('Autoencoder\\n(2D)', fontsize=12, rotation=0, ha='right')\n",
    "\n",
    "plt.suptitle('Reconstruction Comparison', fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Key Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of reconstruction errors\n",
    "print(\"=\"*60)\n",
    "print(\"RECONSTRUCTION ERROR COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PCA (50 components):        MSE = {pca_mse:.6f}\")\n",
    "print(f\"Factor Analysis (50 comp):  MSE = {fa_mse:.6f}\")\n",
    "print(f\"Autoencoder (2D encoding):  MSE = {ae_mse:.6f}\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"KEY DIFFERENCES:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"PCA:\")\n",
    "print(\"  • Linear transformation\")\n",
    "print(\"  • Maximizes variance\")\n",
    "print(\"  • Orthogonal components\")\n",
    "print(\"  • Fast and deterministic\")\n",
    "print()\n",
    "print(\"Factor Analysis:\")\n",
    "print(\"  • Assumes latent factors + noise\")\n",
    "print(\"  • Models correlations between features\")\n",
    "print(\"  • Factors need not be orthogonal\")\n",
    "print(\"  • Provides noise variance estimates\")\n",
    "print()\n",
    "print(\"Autoencoder:\")\n",
    "print(\"  • Nonlinear transformation\")\n",
    "print(\"  • Learns hierarchical features\")\n",
    "print(\"  • More flexible but computationally expensive\")\n",
    "print(\"  • Requires training (non-deterministic)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. When to Use Each Method\n",
    "\n",
    "**Use PCA when:**\n",
    "- You need a fast, deterministic solution\n",
    "- The data has linear structure\n",
    "- You want to understand variance explained\n",
    "- Orthogonal components are desired\n",
    "\n",
    "**Use Factor Analysis when:**\n",
    "- You want to model latent factors\n",
    "- Understanding noise structure is important\n",
    "- You need a probabilistic interpretation\n",
    "- Data has correlations you want to explain\n",
    "\n",
    "**Use Autoencoders when:**\n",
    "- Data has complex, nonlinear structure\n",
    "- You have sufficient training data\n",
    "- Computational resources allow training\n",
    "- You need very low-dimensional representations\n",
    "- Flexibility in architecture is valuable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
